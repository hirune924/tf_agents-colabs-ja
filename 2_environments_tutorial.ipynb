{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ma19Ks2CTDbZ"
   },
   "source": [
    "##### Copyright 2018 The TF-Agents Authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4_16bQF0anmE"
   },
   "source": [
    "### Get Started\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/agents/blob/master/tf_agents/colabs/2_environments_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/agents/blob/master/tf_agents/colabs/2_environments_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KKU2iY_7at8Y"
   },
   "outputs": [],
   "source": [
    "# Note: If you haven't installed tf-agents or gym yet, run:\n",
    "!pip install tf-agents-nightly\n",
    "!pip install tf-nightly\n",
    "!pip install 'gym==0.10.11'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6J5G3q-xa3Ag"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1ZAoFNwnRbKK"
   },
   "outputs": [],
   "source": [
    "import abc\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.environments import time_step as ts\n",
    "from tf_agents.environments import wrappers\n",
    "from tf_agents.environments import suite_gym\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9h3B-YBHopJI"
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n9c6vCPGovOM"
   },
   "source": [
    "強化学習（RL）の目的は、環境と作用することによって学習するAgentを設計することです。標準のRL設定では、Agentはタイムステップごとに観測値を受け取り、行動を選択します。行動は環境に適用され、環境は報酬とそして新しい観察を返します。Agentは利益の合計（returnとも言う）を最大化するための行動を選択するための方策を学習します。\n",
    "\n",
    "TF-Agentでは、環境はPythonまたはTensorFlowのどちらでも実装できます。Python環境は通常、実装、理解、デバッグが簡単ですが、TensorFlow環境はより効率的で自然な並列化が可能です。最も一般的なワークフローは、Pythonで環境を実装し、それを自動的にTensorFlowに変換するためのラッパーの1つを使用することです。\n",
    "\n",
    "まずPythonの環境を見てみましょう。 TensorFlow環境は非常によく似たAPIに従います。\n",
    "\n",
    "---\n",
    "\n",
    "The goal of Reinforcement Learning (RL) is to design agents that learn by interacting with an environment. In the standard RL setting, the agent receives an observation at every time step and chooses an action. The action is applied to the environment and the environment returns a reward and and a new observation. The agent trains a policy to choose actions to maximize the sum of rewards, also known as return.\n",
    "\n",
    "In TF-Agents, environments can be implemented either in Python or TensorFlow. Python environments are usually easier to implement, understand or debug, but TensorFlow environments are more efficient and allow natural parallelization. The most common workflow is to implement an environment in Python and use one of our wrappers to automatically convert it into TensorFlow.\n",
    "\n",
    "Let us look at Python environments first. TensorFlow environments follow a very similar API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x-y4p9i9UURn"
   },
   "source": [
    "# Python Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JPSwHONKMNv9"
   },
   "source": [
    "Python環境には、アクションを環境に適用して次のステップに関する次の情報を返す `step(action) -> next_time_step`メソッドがあります。\n",
    "1. `observation`: これは、Agentが次のステップでその行動を選択するために観察することができる環境状態の一部である。\n",
    "2. `reward`: Agentは学習ステップを通してこれらの報酬の合計を最大化することを学んでいます。\n",
    "3. `step_type`: 環境との相互作用は、通常、例えばチェスのゲームの進行のようにシーケンス/エピソードの一部です。step_typeは、このタイムステップがシーケンスの最初のステップ、中間のステップ、最後のステップのどれであるか示すために、`FIRST`、`MID`または `LAST`のいずれかです。\n",
    "4. `discount`: これは、現在のタイムステップでの報酬に対する、次のタイムステップでの報酬の割引率を表すfloat値です。\n",
    "\n",
    "これらは名前付きタプル`TimeStep(step_type, reward, discount, observation)`にまとめられています。\n",
    "\n",
    "python環境が実装しなければならないインターフェースは`environments/py_environment.PyEnvironment`です。基本的な方法は次のとおりです。\n",
    "\n",
    "---\n",
    "\n",
    "Python environments have a `step(action) -> next_time_step` method that applies an action to the environment, and returns the following information about the next step:\n",
    "1. `observation`: This is the part of the environment state that the agent can observe to choose its actions at the next step.\n",
    "2. `reward`: The agent is learning to maximize the sum of these rewards across multiple steps.\n",
    "3. `step_type`: Interactions with the environment are usually part of a sequence/episode. e.g. multiple moves in a game of chess. step_type can be either `FIRST`, `MID` or `LAST` to indicate whether this time step is the first, intermediate or last step in a sequence.\n",
    "4. `discount`: This is a float representing how much to weight the reward at the next time step relative to the reward at the current time step.\n",
    "\n",
    "These are grouped into a named tuple `TimeStep(step_type, reward, discount, observation)`.\n",
    "\n",
    "The interface that all python environments must implement is in `environments/py_environment.PyEnvironment`. The main methods are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GlD2Dd2vUTtg"
   },
   "outputs": [],
   "source": [
    "class PyEnvironment(object):\n",
    "\n",
    "  def reset(self):\n",
    "    \"\"\"Return initial_time_step.\"\"\"\n",
    "    self._current_time_step = self._reset()\n",
    "    return self._current_time_step\n",
    "\n",
    "  def step(self, action):\n",
    "    \"\"\"Apply action and return new time_step.\"\"\"\n",
    "    self._current_time_step = self._reset()\n",
    "    return self._current_time_step\n",
    "\n",
    "  def current_time_step(self):\n",
    "    return self._current_time_step\n",
    "\n",
    "  def time_step_spec(self):\n",
    "    \"\"\"Return time_step_spec.\"\"\"\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def observation_spec(self):\n",
    "    \"\"\"Return observation_spec.\"\"\"\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def action_spec(self):\n",
    "    \"\"\"Return action_spec.\"\"\"\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def _reset(self):\n",
    "    \"\"\"Return initial_time_step.\"\"\"\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def _step(self, action):\n",
    "    \"\"\"Apply action and return new time_step.\"\"\"\n",
    "    self._current_time_step = self._step(action)\n",
    "    return self._current_time_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zfF8koryiGPR"
   },
   "source": [
    "`step()`メソッドに加えて、環境は新しいシーケンスを開始し初期の`TimeStep`を設定する`reset()`メソッドも提供します。ですが明示的に`reset`メソッドを呼び出す必要はありません。エピソードが終了するか、`step()`が最初に呼び出されたときに、環境は自動的にリセットされます。\n",
    "\n",
    "サブクラスは`step()`や`reset()`を直接実装していないことに注意してください。それらは代わりに`_step()`と`_reset()`メソッドをオーバーライドします。これらのメソッドから返されたタイムステップはキャッシュされ`current_time_step()`を通して使用されます。\n",
    "\n",
    "`observation_spec`メソッドと`action_spec`メソッドは、それぞれの名前と形状、データ型、そして観測とアクションの範囲を記述する`(Bounded)ArraySpecs`のネストを返します。\n",
    "\n",
    "TF-Agentsでは、リスト、タプル、名前付きタプル、または辞書で構成される構造のようなツリーとして定義されるネストを繰り返し参照します。これらは観察や行動の構造を維持するために任意に構成することができます。これは多くの観察と行動をするような、より複雑な環境を表現するのに非常に有用であるとわかりました。\n",
    "\n",
    "---\n",
    "\n",
    "In addition to the `step()` method, environments also provide a `reset()` method that starts a new sequence and provides an initial `TimeStep`. It is not necessary to call the `reset` method explicitly. We assume that environments reset automatically, either when they get to the end of an episode or when step() is called the first time.\n",
    "\n",
    "Note that subclasses do not implement `step()` or `reset()` directly. They instead override the `_step()` and `_reset()` methods. The time steps returned from  these methods will be cached and exposed through `current_time_step()`.\n",
    "\n",
    "The `observation_spec` and the `action_spec` methods return a nest of `(Bounded)ArraySpecs` that describe the name, shape, datatype and ranges of the observations and actions respectively.\n",
    "\n",
    "In TF-Agents we repeatedly refer to nests which are defined as any tree like structure composed of lists, tuples, named-tuples, or dictionaries. These can be arbitrarily composed to maintain structure of observations and actions. We have found this to be very useful for more complex environments where you have many observations and actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r63R-RbjcIRw"
   },
   "source": [
    "## Using Standard Environments\n",
    "\n",
    "TF-AgentsはOpenAI Gym、DeepMind-control、Atariのような多くの標準的な環境のための組み込みラッパーを持っており、それらは`py_environment.PyEnvironment`インターフェースに従います。これらのラップされた環境は、私たちの環境suitesを使って簡単にロードすることができます。OpenAIジムからCartPole環境をロードし、actionとtime_step_specを見てみましょう。\n",
    "\n",
    "---\n",
    "\n",
    "TF Agents has built-in wrappers for many standard environments like the OpenAI Gym, DeepMind-control and Atari, so that they follow our `py_environment.PyEnvironment` interface. These wrapped evironments can be easily loaded using our environment suites. Let's load the CartPole environment from the OpenAI gym and look at the action and time_step_spec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1kBPE5T-nb2-"
   },
   "outputs": [],
   "source": [
    "environment = suite_gym.load('CartPole-v0')\n",
    "print 'action_spec:', environment.action_spec()\n",
    "print 'time_step_spec.observation:', environment.time_step_spec().observation\n",
    "print 'time_step_spec.step_type:', environment.time_step_spec().step_type\n",
    "print 'time_step_spec.discount:', environment.time_step_spec().discount\n",
    "print 'time_step_spec.reward:', environment.time_step_spec().reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vWXOC863Apo_"
   },
   "source": [
    "環境は[0、1]の型 `int64`のアクションが与えられ、`TimeSteps`を返すことがわかります。（観測値は長さ4の`float32`ベクトルで、割引係数は[0.0、1.0]の`float32`スカラーです）それでは、エピソード全体に対して固定アクション`(1,)`を試してみましょう。\n",
    "\n",
    "---\n",
    "\n",
    "So we see that the environment expects actions of type `int64` in [0, 1] and returns `TimeSteps` where the observations are a `float32` vector of length 4 and discount factor is a `float32` in [0.0, 1.0]. Now, let's try to take a fixed action `(1,)` for a whole episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AzIbOJ0-0y12"
   },
   "outputs": [],
   "source": [
    "action = 1\n",
    "time_step = environment.reset()\n",
    "print time_step\n",
    "while not time_step.is_last():\n",
    "  time_step = environment.step(action)\n",
    "  print time_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4xAbBl4_PMtA"
   },
   "source": [
    "## Creating your own Python Environment\n",
    "\n",
    "多くのユーザーにとって、一般的なユースケースは様々な問題にTF-Agentsの標準的なエージェント（agents/ を参照）のうちの1つを適用することです。これをするために、彼らは環境として彼らの問題を組み立てる必要があります。それでは、Pythonで環境を実装する方法を見てみましょう。\n",
    "\n",
    "次のブラックジャックのようなカードゲームをプレイするようにエージェントを訓練したいとしましょう。\n",
    "1. ゲームは1 ... 10の番号が付いたカードの無限のデッキを使ってプレイされます。\n",
    "2. 毎ターン、Agentは2種類のアクションを取ることができます。（新しいランダムなカードを入手するか、現在のラウンドを中止する。）\n",
    "3. 目標は、ラウンドの終わりにカードの合計を21を超えることなく、21にできるだけ近づけることです。\n",
    "\n",
    "ゲームを表す環境は次のようになります。\n",
    "\n",
    "1. Actions: 2つのアクションがあります。アクション0：新しいカードを入手し、アクション1：現在のラウンドを終了します。\n",
    "2. Observations: 現在のラウンドのカードの合計\n",
    "3. Reward: 目的はできるだけ21に近づけることです。そのためラウンドの最後に次の報酬を使用してこれを達成できます。`sum_of_cards - 21 if sum_of_cards <= 21 else -21`\n",
    "\n",
    "---\n",
    "\n",
    "For many clients, a common use case is to apply one of the standard agents (see agents/) in TF-Agents to their problem. To do this, they have to frame their problem as an environment. So let us look at how to implement an environment in Python.\n",
    "\n",
    "Let's say we want to train an agent to play the following (Black Jack inspired) card game:\n",
    "\n",
    "1. The game is played using an infinite deck of cards numbered 1...10.\n",
    "2. At every turn the agent can do 2 things: get a new random card, or stop the current round.\n",
    "3. The goal is to get the sum of your cards as close to 21 as possible at the end of the round, without going over.\n",
    "\n",
    "An environment that represents the game could look like this:\n",
    "\n",
    "1. Actions: We have 2 actions. Action 0: get a new card, and Action 1: terminate the current round.\n",
    "2. Observations: Sum of the cards in the current round.\n",
    "3. Reward: The objective is to get as close to 21 as possible without going over, so we can achieve this using the following reward at the end of the round:\n",
    "   sum_of_cards - 21 if sum_of_cards <= 21, else -21\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9HD0cDykPL6I"
   },
   "outputs": [],
   "source": [
    "class CardGameEnv(py_environment.PyEnvironment):\n",
    "\n",
    "  def __init__(self):\n",
    "    self._action_spec = array_spec.BoundedArraySpec(\n",
    "        shape=(), dtype=np.int32, minimum=0, maximum=1, name='action')\n",
    "    self._observation_spec = array_spec.BoundedArraySpec(\n",
    "        shape=(1,), dtype=np.int32, minimum=0, name='observation')\n",
    "    self._state = 0\n",
    "    self._episode_ended = False\n",
    "\n",
    "  def action_spec(self):\n",
    "    return self._action_spec\n",
    "\n",
    "  def observation_spec(self):\n",
    "    return self._observation_spec\n",
    "\n",
    "  def _reset(self):\n",
    "    self._state = 0\n",
    "    self._episode_ended = False\n",
    "    return ts.restart(np.array([self._state], dtype=np.int32))\n",
    "\n",
    "  def _step(self, action):\n",
    "\n",
    "    if self._episode_ended:\n",
    "      # The last action ended the episode. Ignore the current action and start\n",
    "      # a new episode.\n",
    "      return self.reset()\n",
    "\n",
    "    # Make sure episodes don't go on forever.\n",
    "    if action == 1:\n",
    "      self._episode_ended = True\n",
    "    elif action == 0:\n",
    "      new_card = np.random.randint(1, 11)\n",
    "      self._state += new_card\n",
    "    else:\n",
    "      raise ValueError('`action` should be 0 or 1.')\n",
    "\n",
    "    if self._episode_ended or self._state >= 21:\n",
    "      reward = self._state - 21 if self._state <= 21 else -21\n",
    "      return ts.termination(np.array([self._state], dtype=np.int32), reward)\n",
    "    else:\n",
    "      return ts.transition(\n",
    "          np.array([self._state], dtype=np.int32), reward=0.0, discount=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LYEwyX7QsqeX"
   },
   "source": [
    "上記の環境を正しく定義するためにすべての作業を行ったことを確認しましょう。独自の環境を作成するときは、生成される観測とtime_stepsが、仕様で定義されている正しい形状と型に従っていることを確認する必要があります。これらはTensorFlowグラフを生成するために使用されるので、間違っているとデバッグが困難になる可能性があります。\n",
    "\n",
    "環境を検証するために、ランダムポリシーを使用してアクションを生成し、5つのエピソードを繰り返して、実装が意図したとおりに機能していることを確認します。環境仕様に従わないtime_stepを受け取った場合はエラーが発生します。\n",
    "\n",
    "---\n",
    "\n",
    "Let's make sure we did everything correctly defining the above environment. When creating your own environment you must make sure the observations and time_steps generated follow the correct shapes and types as defined in your specs. These are used to generate the TensorFlow graph and as such can create hard to debug problems if we get them wrong.\n",
    "\n",
    "To validate our environment we will use a random policy to generate actions and we will iterate over 5 episodes to make sure things are working as intended. An error is raised if we receive a time_step that does not follow the environment specs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Hhm-5R7spVx"
   },
   "outputs": [],
   "source": [
    "environment = CardGameEnv()\n",
    "utils.validate_py_environment(environment, episodes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_36eM7MvkNOg"
   },
   "source": [
    "環境が意図したとおりに機能していることがわかったので、固定のポリシーを使用してこの環境を実行しましょう。3枚の新しいカードを要求してからラウンドを終了します。\n",
    "\n",
    "---\n",
    "\n",
    "Now that we know the environment is working as intended, let's run this environment using a fixed policy: ask for 3 cards and then end the round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FILylafAkMEx"
   },
   "outputs": [],
   "source": [
    "get_new_card_action = 0\n",
    "end_round_action = 1\n",
    "\n",
    "environment = CardGameEnv()\n",
    "time_step = environment.reset()\n",
    "print time_step\n",
    "cumulative_reward = time_step.reward\n",
    "\n",
    "for _ in range(3):\n",
    "  time_step = environment.step(get_new_card_action)\n",
    "  print time_step\n",
    "  cumulative_reward += time_step.reward\n",
    "\n",
    "time_step = environment.step(end_round_action)\n",
    "print time_step\n",
    "cumulative_reward += time_step.reward\n",
    "print 'Final Reward = ', cumulative_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_vBLPN3ioyGx"
   },
   "source": [
    "## Environment Wrappers\n",
    "\n",
    "環境ラッパーはpython環境を取り、環境の修正版を返します。元の環境と変更された環境はどちらも`py_environment.PyEnvironment`のインスタンスであり、複数のラッパーを連鎖的に被せることができます。\n",
    "\n",
    "いくつかの一般的なラッパーは`environments/wrappers.py`にあります。例えば：\n",
    "1. `ActionDiscretizeWrapper`: 連続アクション空間を離散アクション空間に変換します。\n",
    "2. `RunStatsWrapper`: 取得したステップ数、完了したエピソード数など、環境の統計情報を取得します。\n",
    "3. `TimeLimitWrapper`: 一定数のステップの後にエピソードを終了します。\n",
    "4. `VideoWrapper`: 環境のビデオをキャプチャします。\n",
    "\n",
    "---\n",
    "\n",
    "An environment wrapper takes a python environment and returns a modified version of the environment. Both the original environment and the modified environment are instances of `py_environment.PyEnvironment`, and multiple wrappers can be chained together.\n",
    "\n",
    "Some common wrappers can be found in `environments/wrappers.py`. For example:\n",
    "\n",
    "1. `ActionDiscretizeWrapper`: Converts a continuous action space to a discrete action space.\n",
    "2. `RunStatsWrapper`: Captures run statistics of the environment such as number of steps taken, number of episodes completed etc.\n",
    "3. `TimeLimitWrapper`: Terminates the episode after a fixed number of steps.\n",
    "4. `VideoWrapper`: Captures a video of the environment.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_8aIybRdnFfb"
   },
   "source": [
    "### Example 1: Action Discretize Wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YIaxJRUpvfyc"
   },
   "source": [
    "InvertedPendulumは `[-1、1]`の範囲で連続的なアクションを受け付けるPyBullet環境です。この環境でDQNなどのAgentをトレーニングしたい場合は、アクション空間を離散化（量子化）する必要があります。これがまさに `ActionDiscretizeWrapper`がすることです。折り返し前後の`action_spec`を比較してください：\n",
    "\n",
    "---\n",
    "\n",
    "InvertedPendulum is a PyBullet environment that accepts continuous actions in the range `[-1, 1]`. If we want to train a discrete action agent such as DQN on this environment, we have to discretize (quantize) the action space. This is exactly what the `ActionDiscretizeWrapper` does. Compare the `action_spec` before and after wrapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AJxEoZ4HoyjR"
   },
   "outputs": [],
   "source": [
    "env = suite_gym.load('Pendulum-v0')\n",
    "print 'Action Spec:', env.action_spec()\n",
    "\n",
    "discrete_action_env = wrappers.ActionDiscretizeWrapper(env, num_actions=5)\n",
    "print 'Discretized Action Spec:', discrete_action_env.action_spec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "njFjW8bmwTWJ"
   },
   "source": [
    "ラップされた`discrete_action_env`は`py_environment.PyEnvironment`のインスタンスであり、通常のpython環境のように扱うことができます。\n",
    "\n",
    "---\n",
    "\n",
    "The wrapped `discrete_action_env` is an instance of `py_environment.PyEnvironment` and can be treated like a regular python environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8l5dwAhsP_F_"
   },
   "source": [
    "# TensorFlow Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iZG39AjBkTjr"
   },
   "source": [
    "TF環境用のインターフェースは`environments/tf_environment.TFEnvironment`で定義されており、Python環境と非常によく似ています。TF環境は、python環境といくつかの点で異なります。\n",
    "* それらは配列の代わりにテンソルオブジェクトを生成します\n",
    "* TF環境では、生成されるテンソルにバッチディメンションが追加されます。\n",
    "\n",
    "Python環境をTFEnvsに変換することで、TensorFlowによる操作の並列化が可能になります。例えば、環境からデータを収集して`replay_buffer`に追加する`collect_experience_op`と、`replay_buffer`から読み込んでAgentをトレーニングする` train_op`を定義し、それらをTensorFlowで自然に並列実行することができます。\n",
    "\n",
    "---\n",
    "\n",
    "The interface for TF environments is defined in `environments/tf_environment.TFEnvironment` and looks very similar to the Python environments. TF Environments differ from python envs in a couple of ways:\n",
    "\n",
    "* They generate tensor objects instead of arrays\n",
    "* TF environments add a batch dimension to the tensors generated when compared to the specs. \n",
    "\n",
    "Converting the python environments into TFEnvs allows tensorflow to parellalize operations. For example, one could define a `collect_experience_op` that collects data from the environment and adds to a `replay_buffer`, and a `train_op` that reads from the `replay_buffer` and trains the agent, and run them in parallel naturally in TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WKBDDZqKTxsL"
   },
   "outputs": [],
   "source": [
    "class TFEnvironment(object):\n",
    "\n",
    "  def time_step_spec(self):\n",
    "    \"\"\"Describes the `TimeStep` tensors returned by `step()`.\"\"\"\n",
    "\n",
    "  def observation_spec(self):\n",
    "    \"\"\"Defines the `TensorSpec` of observations provided by the environment.\"\"\"\n",
    "\n",
    "  def action_spec(self):\n",
    "    \"\"\"Describes the TensorSpecs of the action expected by `step(action)`.\"\"\"\n",
    "\n",
    "  def reset(self):\n",
    "    \"\"\"Returns the current `TimeStep` after resetting the Environment.\"\"\"\n",
    "    return self._reset()\n",
    "\n",
    "  def current_time_step(self):\n",
    "    \"\"\"Returns the current `TimeStep`.\"\"\"\n",
    "    return self._current_time_step()\n",
    "\n",
    "  def step(self, action):\n",
    "    \"\"\"Applies the action and returns the new `TimeStep`.\"\"\"\n",
    "    return self._step(action)\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def _reset(self):\n",
    "    \"\"\"Returns the current `TimeStep` after resetting the Environment.\"\"\"\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def _current_time_step(self):\n",
    "    \"\"\"Returns the current `TimeStep`.\"\"\"\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def _step(self, action):\n",
    "    \"\"\"Applies the action and returns the new `TimeStep`.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tFkBIA92ThWf"
   },
   "source": [
    "`current_time_step()`メソッドは現在のtime_stepを返し、必要なら環境を初期化します。\n",
    "\n",
    "`reset()`メソッドは環境を強制的にリセットし、current_stepを返します。\n",
    "\n",
    "`action`が前の`time_step`に依存していない場合、`Graph`モードでは`tf.control_dependency`が必要です。\n",
    "\n",
    "それでは、`TFEnvironments`がどのように作成されるかを見てみましょう。\n",
    "\n",
    "---\n",
    "\n",
    "The `current_time_step()` method returns the current time_step and initializes the environment if needed.\n",
    "\n",
    "The `reset()` method forces a reset in the environment and returns the current_step.\n",
    "\n",
    "If the `action` doesn't depend on the previous `time_step` a `tf.control_dependency` is needed in `Graph` mode.\n",
    "\n",
    "For now, let us look at how `TFEnvironments` are created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S6wS3AaLdVLT"
   },
   "source": [
    "## Creating your own TensorFlow Environment\n",
    "\n",
    "これはPythonで環境を作成するよりも複雑なので、このcolabではカバーしません。exampleはこちらから確認できます。[here](https://github.com/tensorflow/agents/blob/master/tf_agents/environments/tf_environment_test.py)一般的なユースケースはPythonであなたの環境を実装し、`TFPyEnvironment`ラッパーを使ってTensorFlowでそれをラップすることです（下記参照）。\n",
    "\n",
    "---\n",
    "\n",
    "This is more complicated than creating environments in Python, so we will not cover it in this colab. An example is available [here](https://github.com/tensorflow/agents/blob/master/tf_agents/environments/tf_environment_test.py). The more common use case is to implement your environment in Python and wrap it in TensorFlow using our `TFPyEnvironment` wrapper (see below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V_Ny2lb-dU5R"
   },
   "source": [
    "## Wrapping a Python Environment in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lv4-UcurZ8nb"
   },
   "source": [
    "`TFPyEnvironment`ラッパーを使えば、どんなPython環境でもTensorFlow環境に簡単にラップできます。\n",
    "\n",
    "---\n",
    "\n",
    "We can easily wrap any Python environment into a TensorFlow environment using the `TFPyEnvironment` wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UYerqyNfnVRL"
   },
   "outputs": [],
   "source": [
    "env = suite_gym.load('CartPole-v0')\n",
    "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
    "\n",
    "print(isinstance(tf_env, tf_environment.TFEnvironment))\n",
    "print(\"TimeStep Specs:\", tf_env.time_step_spec())\n",
    "print(\"Action Specs:\", tf_env.action_spec())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z3WFrnX9CNpC"
   },
   "source": [
    "specが `(Bounded)TensorSpec`型になったことに注意してください。\n",
    "\n",
    "---\n",
    "\n",
    "Note the specs are now of type: `(Bounded)TensorSpec`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vQPvC1ARYALj"
   },
   "source": [
    "## Usage Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ov7EIrk8dKUU"
   },
   "source": [
    "### Simple Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gdvFqUqbdB7u"
   },
   "outputs": [],
   "source": [
    "env = suite_gym.load('CartPole-v0')\n",
    "\n",
    "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
    "# reset() creates the initial time_step after resetting the environment.\n",
    "time_step = tf_env.reset()\n",
    "num_steps = 3\n",
    "transitions = []\n",
    "reward = 0\n",
    "for i in range(num_steps):\n",
    "  action = tf.constant([i % 2])\n",
    "  # applies the action and returns the new TimeStep.\n",
    "  next_time_step = tf_env.step(action)\n",
    "  transitions.append([time_step, action, next_time_step])\n",
    "  reward += next_time_step.reward\n",
    "  time_step = next_time_step\n",
    "\n",
    "np_transitions = tf.nest.map_structure(lambda x: x.numpy(), transitions)\n",
    "print('\\n'.join(map(str, np_transitions)))\n",
    "print('Total reward:', reward.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kWs48LNsdLnc"
   },
   "source": [
    "### Whole Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t561kUXMk-KM"
   },
   "outputs": [],
   "source": [
    "env = suite_gym.load('CartPole-v0')\n",
    "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
    "\n",
    "time_step = tf_env.reset()\n",
    "rewards = []\n",
    "steps = []\n",
    "num_episodes = 5\n",
    "\n",
    "for _ in range(num_episodes):\n",
    "  episode_reward = 0\n",
    "  episode_steps = 0\n",
    "  while not time_step.is_last():\n",
    "    action = tf.random_uniform([1], 0, 2, dtype=tf.int32)\n",
    "    time_step = tf_env.step(action)\n",
    "    episode_steps += 1\n",
    "    episode_reward += next_time_step.reward.numpy()\n",
    "  rewards.append(episode_reward)\n",
    "  steps.append(episode_steps)\n",
    "\n",
    "num_steps = np.sum(steps)\n",
    "avg_length = np.mean(steps)\n",
    "avg_reward = np.mean(rewards)\n",
    "\n",
    "print('num_episodes:', num_episodes, 'num_steps:', num_steps)\n",
    "print('avg_length', avg_length, 'avg_reward:', avg_reward)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "last_runtime": {
    "build_target": "",
    "kind": "local"
   },
   "name": "TF-Agents Environments Tutorial.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
