{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Pi_B2cvdBiW"
   },
   "source": [
    "##### Copyright 2018 The TF-Agents Authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f5926O3VkG_p"
   },
   "source": [
    "### Get Started\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/agents/blob/master/tf_agents/colabs/3_policies_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/agents/blob/master/tf_agents/colabs/3_policies_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xsLTHlVdiZP3"
   },
   "outputs": [],
   "source": [
    "# Note: If you haven't installed tf-agents yet, run:\n",
    "!pip install tf-nightly\n",
    "!pip install tfp-nightly\n",
    "!pip install tf-agents-nightly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lEgSa5qGdItD"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sdvop99JlYSM"
   },
   "outputs": [],
   "source": [
    "import abc\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.environments import time_step as ts\n",
    "from tf_agents.networks import network\n",
    "\n",
    "from tf_agents.policies import py_policy\n",
    "from tf_agents.policies import random_py_policy\n",
    "from tf_agents.policies import scripted_py_policy\n",
    "\n",
    "from tf_agents.policies import tf_policy\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.policies import actor_policy\n",
    "from tf_agents.policies import q_policy\n",
    "from tf_agents.policies import greedy_policy\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "31uij8nIo5bG"
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PqFn7q5bs3BF"
   },
   "source": [
    "強化学習の用語では、ポリシーは環境からの観察を行動または行動にわたる分布にマッピングします。TF-Agentsでは、環境からの観察は`TimeStep('step_type', 'discount', 'reward', 'observation')`という名前付きタプルに含まれ、ポリシーはアクションにアクションのタイムステップをマッピングします。ほとんどのポリシーは`timestep.observation`を使用し、いくつかのポリシーは`timestep.step_type`を使用します（例えば、ステートフルポリシーのエピソードの始まりで状態をリセットするため）。しかし、`timestep.discount`と`timestep.reward`は通常無視されます。\n",
    "\n",
    "ポリシーは、次のようにTF-Agentsの他のコンポーネントに関連しています。ほとんどのポリシーには、TimeStepsに対するアクションやアクションの分布を計算するためのニューラルネットワークがあります。Agentは、さまざまな目的のために1つ以上のポリシーを含めることができます。訓練されているメインのポリシー、およびデータ収集のためのnoisyなポリシー。ポリシーは保存/復元することができ、データ収集、評価などのためにエージェントとは無関係に使用することができます。\n",
    "\n",
    "いくつかのポリシーはTensorflowで書くほうが簡単です（例えば、ニューラルネットワークを持つもの）、他のものはPythonで書くほうが簡単です（例えば、アクションのスクリプトに従うもの）。そのためTF-Agentsでは、PythonとTensorflowの両方のポリシーを許可しています。さらに、TensorFlowで書かれたポリシーはPython環境で使われなければならないかもしれません、あるいはその逆、例えばTensorFlowポリシーはトレーニングに使用されますが、後で本番のPython環境にデプロイされます。これを簡単にするために、pythonとTensorFlowのポリシー間で変換するためのラッパーを提供しています。\n",
    "\n",
    "もう一つの興味深いクラスは特定の方法で特定のポリシーを変更するポリシーラッパーです。特定の種類のノイズを追加したり、貪欲またはイプシロン貪欲な確率的ポリシーを作成したり、複数のポリシーをランダムに組み合わせたりします。\n",
    "\n",
    "---\n",
    "\n",
    "In Reinforcement  Learning terminology, policies map an observation from the environment to an action or a distribution over actions. In TF-Agents, observations from the environment are contained in a named tuple `TimeStep('step_type', 'discount', 'reward', 'observation')`, and policies map timesteps to actions or distributions over actions. Most policies use  `timestep.observation`, some policies use `timestep.step_type` (e.g. to reset the state at the beginning of an episode in stateful policies), but `timestep.discount` and `timestep.reward` are usually ignored.\n",
    "\n",
    "Policies are related to other components in TF-Agents in the following way. Most policies have a neural network to compute actions and/or distributions over actions from TimeSteps. Agents can contain one or more policies for different purposes, e.g. a main policy that is being trained for deployment, and a noisy policy for data collection. Policies can be saved/restored, and can be used indepedently of the agent for data collection, evaluation etc.\n",
    "\n",
    "Some policies are easier to write in Tensorflow (e.g. those with a neural network), whereas others are easier to write in Python (e.g. following a script of actions). So in TF agents, we allow both Python and Tensorflow policies. Morever, policies written in TensorFlow might have to be used in a Python environment, or vice versa, e.g. a TensorFlow policy is used for training but later deployed in a production python environment. To make this easier, we provide wrappers for converting between python and TensorFlow policies.\n",
    "\n",
    "Another interesting class of policies are policy wrappers, which modify a given policy in a certain way, e.g. add a particular type of noise, make a greedy or epsilon-greedy version of a stochastic policy, randomly mix multiple policies etc.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NyXO5-Aalb-6"
   },
   "source": [
    "# Python Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DOtUZ1hs02bu"
   },
   "source": [
    "Pythonポリシー用のインターフェースは `policies/py_policy.Base`で定義されています。主な方法は次のとおりです。\n",
    "\n",
    "---\n",
    "\n",
    "The interface for Python policies is defined in `policies/py_policy.Base`. The main methods are:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4PqNEVls1uqc"
   },
   "outputs": [],
   "source": [
    "class Base(object):\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def __init__(self, time_step_spec, action_spec, policy_state_spec=()):\n",
    "    self._time_step_spec = time_step_spec\n",
    "    self._action_spec = action_spec\n",
    "    self._policy_state_spec = policy_state_spec\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def reset(self, policy_state=()):\n",
    "    # return initial_policy_state.\n",
    "    pass\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def action(self, time_step, policy_state=()):\n",
    "    # return a PolicyStep(action, state, info) named tuple.\n",
    "    pass\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def distribution(self, time_step, policy_state=()):\n",
    "    # Not implemented in python, only for TF policies.\n",
    "    pass\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def update(self, policy):\n",
    "    # update self to be similar to the input `policy`.\n",
    "    pass\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def copy(self):\n",
    "    # return a copy of self.\n",
    "    pass\n",
    "\n",
    "  @property\n",
    "  def time_step_spec(self):\n",
    "    return self._time_step_spec\n",
    "\n",
    "  @property\n",
    "  def action_spec(self):\n",
    "    return self._action_spec\n",
    "\n",
    "  @property\n",
    "  def policy_state_spec(self):\n",
    "    return self._policy_state_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "16kyDKk65bka"
   },
   "source": [
    "最も重要な方法は`action(time_step)`で、これは環境からの観測を含む`time_step`を以下の属性を含むtupleという名前のPolicyStepにマッピングします。\n",
    "\n",
    "*  `action`: 環境に適用されるアクション。\n",
    "*  `state`: アクションの次の呼び出しにフィードされるポリシーの状態（たとえば、RNN）。\n",
    "*  `info`: アクションログの確率などのオプションの補足情報。\n",
    "\n",
    "\n",
    "`time_step_spec`と`action_spec`は入力タイムステップと出力アクションの仕様です。ポリシーには、ステートフルポリシーの状態をリセットするために通常使用される「リセット」機能もあります。`copy`関数は`self`のコピーを返し、そして`update(new_policy)`関数は`new_policy`に向かって`self`を更新します。\n",
    "\n",
    "それでは、Pythonポリシーの例をいくつか見てみましょう。\n",
    "\n",
    "---\n",
    "\n",
    "The most important method is `action(time_step)` which maps a `time_step` containing an observation from the environment to a PolicyStep named tuple containing the following attributes:\n",
    "\n",
    "*  `action`: The action to be applied to the environment.\n",
    "*  `state`: The state of the policy (e.g. RNN state) to be fed into the next call to action.\n",
    "*  `info`: Optional side information such as action log probabilities.\n",
    "\n",
    "The `time_step_spec` and `action_spec` are specifications for the input time step and the output action. Policies also have a `reset` function which is typically used for resetting the state in stateful policies. The `copy` function returns a copy of `self` and the `update(new_policy)` function updates `self` towards `new_policy`.\n",
    "\n",
    "Now, let us look at a couple of examples of python policies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YCH1Hs_WlmDT"
   },
   "source": [
    "## Example 1: Random Python Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lbnQ0BQ3_0N2"
   },
   "source": [
    "`PyPolicy`の簡単な例は`RandomPyPolicy`です。これは離散的/連続的な与えられたaction_specに対してランダムなアクションを生成します。入力`time_step`は無視されます。\n",
    "\n",
    "---\n",
    "\n",
    "A simple example of a `PyPolicy` is the `RandomPyPolicy` which generates random actions for the discrete/continuous given action_spec. The input `time_step` is ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QX8M4Nl-_0uu"
   },
   "outputs": [],
   "source": [
    "action_spec = array_spec.BoundedArraySpec((2,), np.int32, -10, 10)\n",
    "my_random_py_policy = random_py_policy.RandomPyPolicy(time_step_spec=None,\n",
    "    action_spec=action_spec)\n",
    "time_step = None\n",
    "action_step = my_random_py_policy.action(time_step)\n",
    "print(action_step)\n",
    "action_step = my_random_py_policy.action(time_step)\n",
    "print(action_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B8WrFOR1lz31"
   },
   "source": [
    "## Example 2: Scripted Python Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AJ0Br1lGBnTT"
   },
   "source": [
    "スクリプト化されたポリシーは`(num_repeats, action)`タプルのリストとして表されるアクションのスクリプトを再生します。`action`関数が呼び出されるたびに、指定された回数の繰り返しが行われるまでリストから次のアクションを返し、それからリストの次のアクションに進みます。リストの先頭から実行を開始するために`reset`メソッドを呼び出すことができます。\n",
    "\n",
    "---\n",
    "\n",
    "A scripted policy plays back a script of actions represented as a list of `(num_repeats, action)` tuples. Every time the `action` function is called, it returns the next action from the list until the specified number of repeats is done, and then moves on to the next action in the list. The `reset` method can be called to start executing from the beginning of the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_mZ244m4BUYv"
   },
   "outputs": [],
   "source": [
    "action_spec = array_spec.BoundedArraySpec((2,), np.int32, -10, 10)\n",
    "action_script = [(1, np.array([5, 2], dtype=np.int32)), \n",
    "                 (0, np.array([0, 0], dtype=np.int32)), # Setting `num_repeates` to 0 will skip this action.\n",
    "                 (2, np.array([1, 2], dtype=np.int32)), \n",
    "                 (1, np.array([3, 4], dtype=np.int32))]\n",
    "\n",
    "my_scripted_py_policy = scripted_py_policy.ScriptedPyPolicy(\n",
    "    time_step_spec=None, action_spec=action_spec, action_script=action_script)\n",
    "\n",
    "policy_state = my_scripted_py_policy.get_initial_state()\n",
    "time_step = None\n",
    "print('Executing scripted policy...')\n",
    "action_step = my_scripted_py_policy.action(time_step, policy_state)\n",
    "print(action_step)\n",
    "action_step= my_scripted_py_policy.action(time_step, action_step.state)\n",
    "print(action_step)\n",
    "action_step = my_scripted_py_policy.action(time_step, action_step.state)\n",
    "print(action_step)\n",
    "\n",
    "print('Resetting my_scripted_py_policy...')\n",
    "policy_state = my_scripted_py_policy.get_initial_state()\n",
    "action_step = my_scripted_py_policy.action(time_step, policy_state)\n",
    "print(action_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Dz7HSTZl6aU"
   },
   "source": [
    "# TensorFlow Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LwcoBXqKl8Yb"
   },
   "source": [
    "TensorFlowポリシーはPythonポリシーと同じインターフェースに従います。いくつか例を見てみましょう。\n",
    "\n",
    "---\n",
    "\n",
    "TensorFlow policies follow the same interface as Python policies. Let us look at a few examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3x8pDWEFrQ5C"
   },
   "source": [
    "## Example 1: Random TF Policy\n",
    "\n",
    "RandomTFPolicyは、与えられた離散的/連続的な`action_spec`に従ってランダムなアクションを生成するために使うことができます。入力`time_step`は無視されます。\n",
    "\n",
    "---\n",
    "\n",
    "A RandomTFPolicy can be used to generate random actions according to a given discrete/continuous `action_spec`. The input `time_step` is ignored.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nZ3pe5G4rjrW"
   },
   "outputs": [],
   "source": [
    "action_spec = tensor_spec.BoundedTensorSpec(\n",
    "    (2,), tf.float32, minimum=-1, maximum=3)\n",
    "input_tensor_spec = tensor_spec.TensorSpec((2,), tf.float32)\n",
    "time_step_spec = ts.time_step_spec(input_tensor_spec)\n",
    "\n",
    "my_random_tf_policy = random_tf_policy.RandomTFPolicy(\n",
    "    action_spec=action_spec, time_step_spec=time_step_spec)\n",
    "observation = tf.ones(time_step_spec.observation.shape)\n",
    "time_step = ts.restart(observation)\n",
    "action_step = my_random_tf_policy.action(time_step)\n",
    "\n",
    "print('Action:')\n",
    "print(action_step.action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GOBoWETprWCB"
   },
   "source": [
    "## Example 2: Actor Policy\n",
    "\n",
    "アクターポリシーは`time_steps`をアクションにマッピングするネットワークか`time_steps`をアクションの分布にマッピングするネットワークのどちらかを使って作成できます。\n",
    "\n",
    "---\n",
    "\n",
    "An actor policy can be created using either a network that maps `time_steps` to actions or a network that maps `time_steps` to distributions over actions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2S94E5zQgge_"
   },
   "source": [
    "### Using an action network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X2LM5STNgv1u"
   },
   "source": [
    "次のようにネットワークを定義しましょう。\n",
    "\n",
    "---\n",
    "\n",
    "Let us define a network as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S2wFgzJFteQX"
   },
   "outputs": [],
   "source": [
    "class ActionNet(network.Network):\n",
    "\n",
    "  def __init__(self, input_tensor_spec, output_tensor_spec):\n",
    "    super(ActionNet, self).__init__(\n",
    "        input_tensor_spec=input_tensor_spec,\n",
    "        state_spec=(),\n",
    "        name='ActionNet')\n",
    "    self._output_tensor_spec = output_tensor_spec\n",
    "    self._layers = [\n",
    "        tf.keras.layers.Dense(\n",
    "            action_spec.shape.num_elements(), activation=tf.nn.tanh),\n",
    "    ]\n",
    "\n",
    "  def call(self, observations, step_type, network_state):\n",
    "    del step_type\n",
    "\n",
    "    output = tf.cast(observations, dtype=tf.float32)\n",
    "    for layer in self.layers:\n",
    "      output = layer(output)\n",
    "    actions = tf.reshape(output, [-1] + self._output_tensor_spec.shape.as_list())\n",
    "\n",
    "    # Scale and shift actions to the correct range if necessary.\n",
    "    return actions, network_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k7fIn-ybVdC6"
   },
   "source": [
    "TensorFlowでは、ほとんどのネットワークレイヤはバッチ処理用に設計されているため、入力time_stepsがバッチ処理され、ネットワークの出力もバッチ処理されることになります。また、ネットワークは与えられたaction_specの正しい範囲でアクションを生成する必要があります。\n",
    "\n",
    " これは慣習的に[-1、1]のアクションを生成するために最終レイヤのためのtanhアクティベーションを使用して行われ、そしてそれをスケーリングして入力action_specとして正しい範囲にシフトします。（例えば `tf_agents/agents/ddpg/networks.actor_network()`を見てください）\n",
    "\n",
    "さて、私たちは上記のネットワークを使ってアクターポリシーを作成します。\n",
    "\n",
    "---\n",
    "\n",
    "In TensorFlow most network layers are designed for batch operations, so we expect the input time_steps to be batched, and the output of the network will be batched as well. Also the network is responsible for producing actions in the correct range of the given action_spec. This is conventionally done using e.g. a tanh activation for the final layer to produce actions in [-1, 1] and then scaling and shifting this to the correct range as the input action_spec (e.g. see `tf_agents/agents/ddpg/networks.actor_network()`).\n",
    "\n",
    "Now, we an create an actor policy using the above network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0UGmFTe7a5VQ"
   },
   "outputs": [],
   "source": [
    "input_tensor_spec = tensor_spec.TensorSpec((4,), tf.float32)\n",
    "time_step_spec = ts.time_step_spec(input_tensor_spec)\n",
    "action_spec = tensor_spec.BoundedTensorSpec((3,),\n",
    "                                            tf.float32,\n",
    "                                            minimum=-1,\n",
    "                                            maximum=1)\n",
    "\n",
    "action_net = ActionNet(input_tensor_spec, action_spec)\n",
    "\n",
    "my_actor_policy = actor_policy.ActorPolicy(\n",
    "    time_step_spec=time_step_spec,\n",
    "    action_spec=action_spec,\n",
    "    actor_network=action_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xlmGPTAmfPK3"
   },
   "source": [
    "time_step_specに続くtime_stepsの任意のバッチにそれを適用することができます。\n",
    "\n",
    "---\n",
    "\n",
    "We can apply it to any batch of time_steps that follow time_step_spec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fvsIsR0VfOA4"
   },
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "observations = tf.ones([2] + time_step_spec.observation.shape.as_list())\n",
    "\n",
    "time_step = ts.restart(observations, batch_size)\n",
    "\n",
    "action_step = my_actor_policy.action(time_step)\n",
    "print('Action:')\n",
    "print(action_step.action)\n",
    "\n",
    "distribution_step = my_actor_policy.distribution(time_step)\n",
    "print('Action distribution:')\n",
    "print(distribution_step.action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lumtyhejZOXR"
   },
   "source": [
    "上記の例では、アクションテンソルを生成するアクションネットワークを使用してポリシーを作成しました。この場合、`policy.distribution(time_step)`は`policy.action(time_step)`の出力を中心とした決定論的（デルタ）分布です。確率論的ポリシーを作成する1つの方法は、アクションにノイズを追加するポリシーラッパーでアクターポリシーをラップすることです。 もう1つの方法は、以下に示すように、アクションネットワークではなくAction Distribution Networkを使用してアクターポリシーを作成することです。\n",
    "\n",
    "---\n",
    "\n",
    "In the above example, we created the policy using an action network that produces an action tensor. In this case, `policy.distribution(time_step)` is a deterministic (delta) distribution around the output of `policy.action(time_step)`. One way to produce a stochastic policy is to wrap the actor policy in a policy wrapper that adds noise to the actions. Another way is to create the actor policy using an action distribution network instead of an action network as shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_eNrJ5gKgl3W"
   },
   "source": [
    "### Using an action distribution network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sSYzC9LobVsK"
   },
   "outputs": [],
   "source": [
    "class ActionDistributionNet(ActionNet):\n",
    "\n",
    "  def call(self, observations, step_type, network_state):\n",
    "    action_means, network_state = super(ActionDistributionNet, self).call(\n",
    "        observations, step_type, network_state)\n",
    "\n",
    "    action_std = tf.ones_like(action_means)\n",
    "    return tfp.distributions.Normal(action_means, action_std), network_state\n",
    "\n",
    "\n",
    "action_distribution_net = ActionDistributionNet(input_tensor_spec, action_spec)\n",
    "\n",
    "my_actor_policy = actor_policy.ActorPolicy(\n",
    "    time_step_spec=time_step_spec,\n",
    "    action_spec=action_spec,\n",
    "    actor_network=action_distribution_net)\n",
    "\n",
    "action_step = my_actor_policy.action(time_step)\n",
    "print('Action:')\n",
    "print(action_step.action)\n",
    "distribution_step = my_actor_policy.distribution(time_step)\n",
    "print('Action distribution:')\n",
    "print(distribution_step.action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BzoNGJnlibtz"
   },
   "source": [
    "上記では、アクションは与えられたアクション仕様[-1、1]の範囲に切り取られていることに注意してください。これは、ActorPolicy clipのコンストラクタ引数がデフォルトでTrueであるためです。これをfalseに設定すると、ネットワークによって生成されたクリップされていないアクションが返されます。\n",
    "\n",
    "---\n",
    "\n",
    "Note that in the above, actions are clipped to the range of the given action spec [-1, 1]. This is because a constructor argument of ActorPolicy clip=True by default. Setting this to false will return unclipped actions produced by the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PLj6A-5domNG"
   },
   "source": [
    "確率的ポリシーは、例えば、その動作として `stochastic_policy.distribution().mode()`を選択するGreedyPolicyラッパー、およびその`distribution()`としてのこの欲張り動作の周囲の決定論的/デルタ分布を使用して決定論的ポリシーに変換することができる。\n",
    "\n",
    "---\n",
    "\n",
    "Stochastic policies can be converted to deterministic policies using, for example, a GreedyPolicy wrapper which chooses `stochastic_policy.distribution().mode()` as its action, and a deterministic/delta distribution around this greedy action as its `distribution()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Xxzo2a7rZ7v"
   },
   "source": [
    "## Example 3: Q Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "79eGLqpOhQVp"
   },
   "source": [
    "QポリシーはDQNなどのエージェントで使用され、各個別のアクションのQ値を予測するQネットワークに基づいています。与えられた時間ステップに対してQポリシーのアクション分布はq値をlogitsとして使用して作成されたカテゴリカル分布です。\n",
    "\n",
    "---\n",
    "\n",
    "A Q policy is used in agents like DQN and is based on a Q network that predicts a Q value for each discrete action. For a given time step, the action distribution in the Q Policy is a categorical distribution created using the q values as logits.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Haakr2VvjqKC"
   },
   "outputs": [],
   "source": [
    "input_tensor_spec = tensor_spec.TensorSpec((4,), tf.float32)\n",
    "time_step_spec = ts.time_step_spec(input_tensor_spec)\n",
    "action_spec = tensor_spec.BoundedTensorSpec((1,),\n",
    "                                            tf.int32,\n",
    "                                            minimum=-1,\n",
    "                                            maximum=1)\n",
    "num_actions = action_spec.maximum - action_spec.minimum + 1\n",
    "\n",
    "\n",
    "class QNetwork(network.Network):\n",
    "\n",
    "  def __init__(self, input_tensor_spec, action_spec, num_actions=2, name=None):\n",
    "    super(QNetwork, self).__init__(\n",
    "        input_tensor_spec=input_tensor_spec,\n",
    "        state_spec=(),\n",
    "        name=name)\n",
    "    self._layers.append(tf.keras.layers.Dense(num_actions))\n",
    "\n",
    "  def call(self, inputs, unused_step_type=None, network_state=()):\n",
    "    inputs = tf.cast(inputs, tf.float32)\n",
    "    for layer in self.layers:\n",
    "      inputs = layer(inputs)\n",
    "    return inputs, network_state\n",
    "\n",
    "\n",
    "batch_size = 2\n",
    "observation = tf.ones([batch_size] + time_step_spec.observation.shape.as_list())\n",
    "time_steps = ts.restart(observation, batch_size=batch_size)\n",
    "\n",
    "my_q_network = QNetwork(\n",
    "    input_tensor_spec=input_tensor_spec,\n",
    "    action_spec=action_spec)\n",
    "my_q_policy = q_policy.QPolicy(\n",
    "    time_step_spec, action_spec, q_network=my_q_network)\n",
    "action_step = my_q_policy.action(time_steps)\n",
    "distribution_step = my_q_policy.distribution(time_steps)\n",
    "\n",
    "print('Action:')\n",
    "print(action_step.action)\n",
    "\n",
    "print('Action distribution:')\n",
    "print(distribution_step.action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xpu9m6mvqJY-"
   },
   "source": [
    "# Policy Wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OfaUrqRAoigk"
   },
   "source": [
    "ポリシーラッパーは、与えられたポリシーをラップおよび修正するために使用することができます。（例えばノイズを追加するなど）ポリシーラッパーはPolicy（Python / TensorFlow）のサブクラスであるため、他のポリシーと同じように使用できます。\n",
    "\n",
    "---\n",
    "\n",
    "A policy wrapper can be used to wrap and modify a given policy, e.g. add noise. Policy wrappers are a subclass of Policy (Python/TensorFlow) and can therefore be used just like any other policy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-JJVVAALqVNQ"
   },
   "source": [
    "## Example: Greedy Policy\n",
    "\n",
    "Greedyラッパーは`distribution()`を実装するあらゆるTensorFlowポリシーをラップするために使用することができます。`GreedyPolicy.action()`は `wrapped_policy.distribution().mode()`を返し、`GreedyPolicy.distribution()`は `GreedyPolicy.action()`周りの決定論的/デルタ分布です：\n",
    "\n",
    "---\n",
    "\n",
    "A greedy wrapper can be used to wrap any TensorFlow policy that implements `distribution()`. `GreedyPolicy.action()` will return `wrapped_policy.distribution().mode()` and `GreedyPolicy.distribution()` is a deterministic/delta distribution around `GreedyPolicy.action()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xsRPBeLZtXvu"
   },
   "outputs": [],
   "source": [
    "my_greedy_policy = greedy_policy.GreedyPolicy(my_q_policy)\n",
    "\n",
    "action_step = my_greedy_policy.action(time_steps)\n",
    "print('Action:')\n",
    "print(action_step.action)\n",
    "\n",
    "distribution_step = my_greedy_policy.distribution(time_steps)\n",
    "print('Action distribution:')\n",
    "print(distribution_step.action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TF-Agents Policies Tutorial.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
